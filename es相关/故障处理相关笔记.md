# 问题1：logstash日志有报错（磁盘使用率超过95%）

[logstash.outputs.elasticsearch] retrying failed action with response code: 403 ({"type"=>"cluster_block_exception", "reason"=>"blocked by: [FORBIDDEN/12/index read-only / allow delete (api)];"})

通过网上检查，https://www.aityp.com/%E8%A7%A3%E5%86%B3elasticsearch%E7%B4%A2%E5%BC%95%E5%8F%AA%E8%AF%BB/
确实是因为磁盘空间超过95%了。
也按照这个操作执行了：
curl -XPUT -H 'Content-Type: application/json' http://localhost:9200/_all/_settings -d '{"index.blocks.read_only_allow_delete": null}'

# 问题2：logstash无法写入数据，集群脑裂
突然有一天很奇怪，通过kibana怎么都查不到当天、前一天的数据。集群是三个节点，我反复用logstash去测试，logstash日志也无异常（正常的解析日志貌似没有，
只能看到启动成功了），用ftp把文件传到另外一个服务器上，再去执行logstash解析，还是一样，邪门了。数据是肯定没问题的，毕竟生成json文件的程序没动过。

最后发现是一个节点脱离了集群，我通过kibana去看index，发现状态都是红色的！首先我执行了检查：
* 看看3个es节点是不是服务都在；
* 都在；执行get /_cat/nodes来查看集群状态，发现一个脱离了集群
* 重启了一下那个脱离的集群(node-2），然后好了
## 重启前的日志：
```
[2019-12-03T15:56:52,391][INFO ][o.e.c.r.a.AllocationService] [node-2] Cluster health status changed from [GREEN] to
 [RED] (reason: []).
[2019-12-03T15:56:52,391][INFO ][o.e.c.s.MasterService    ] [node-2] zen-disco-node-failed({node-1}{Vkyd43raSeSCFKp-
iUHG3Q}{dqbtobQhRkyVopOUMZxjhw}{192.168.162.15}{192.168.162.15:9399}{ml.machine_memory=8252608512, ml.max_open_jobs=20
, xpack.installed=true, ml.enabled=true}), reason(transport disconnected), zen-disco-node-failed({node-3}{arPYw1QzR5
uyk6hcanQujw}{StRpghr6Q_y1xE4GU2WSDg}{192.168.161.23}{192.168.161.23:9399}{ml.machine_memory=8252608512, ml.max_open_j
obs=20, xpack.installed=true, ml.enabled=true}), reason(transport disconnected), reason: removed {{node-3}{arPYw1QzR
5uyk6hcanQujw}{StRpghr6Q_y1xE4GU2WSDg}{192.168.161.23}{192.168.161.23:9399}{ml.machine_memory=8252608512, ml.max_open_
jobs=20, xpack.installed=true, ml.enabled=true},{node-1}{Vkyd43raSeSCFKp-iUHG3Q}{dqbtobQhRkyVopOUMZxjhw}{192.168.162.
15}{192.168.162.15:9399}{ml.machine_memory=8252608512, ml.max_open_jobs=20, xpack.installed=true, ml.enabled=true},}
[2019-12-03T15:56:52,392][INFO ][o.e.c.s.ClusterApplierService] [node-2] removed {{node-3}{arPYw1QzR5uyk6hcanQujw}{S
tRpghr6Q_y1xE4GU2WSDg}{192.168.161.23}{192.168.161.23:9399}{ml.machine_memory=8252608512, ml.max_open_jobs=20, xpack.i
nstalled=true, ml.enabled=true},{node-1}{Vkyd43raSeSCFKp-iUHG3Q}{dqbtobQhRkyVopOUMZxjhw}{192.168.162.15}{192.168.162.1
5:9399}{ml.machine_memory=8252608512, ml.max_open_jobs=20, xpack.installed=true, ml.enabled=true},}, reason: apply c
luster state (from master [master {node-2}{Ci-aSglBRKWZM5JkoKvKrg}{wZVFRpTxTf2S6YScHcThwQ}{192.168.162.14}{192.168.162
.14:9399}{ml.machine_memory=8252608512, xpack.installed=true, ml.max_open_jobs=20, ml.enabled=true} committed versio
n [170] source [zen-disco-node-failed({node-1}{Vkyd43raSeSCFKp-iUHG3Q}{dqbtobQhRkyVopOUMZxjhw}{192.168.162.15}{192.168
.162.15:9399}{ml.machine_memory=8252608512, ml.max_open_jobs=20, xpack.installed=true, ml.enabled=true}), reason(tra
nsport disconnected), zen-disco-node-failed({node-3}{arPYw1QzR5uyk6hcanQujw}{StRpghr6Q_y1xE4GU2WSDg}{192.168.161.23}{
192.168.161.23:9399}{ml.machine_memory=8252608512, ml.max_open_jobs=20, xpack.installed=true, ml.enabled=true}), reas
on(transport disconnected)]])
[2019-12-03T15:56:52,425][INFO ][o.e.i.s.IndexShard       ] [node-2] [audit-qizhi-fort_log-2019.08][3] primary-repli
ca resync completed with 0 operations
[2019-12-03T15:56:52,437][INFO ][o.e.i.s.IndexShard       ] [node-2] [audit-qizhi-fort_log-2019.09][3] primary-repli
ca resync completed with 0 operations
```
## 重启后的日志：
```
[2019-12-04T14:49:50,283][INFO ][o.e.x.s.a.TokenService   ] [node-2] refresh keys
[2019-12-04T14:49:50,552][INFO ][o.e.x.s.a.TokenService   ] [node-2] refreshed keys
[2019-12-04T14:49:50,618][INFO ][o.e.l.LicenseService     ] [node-2] license [c03d134c-15a0-45ec-9074-5df9729ccec1]
mode [basic] - valid
[2019-12-04T14:49:50,634][INFO ][o.e.x.s.t.n.SecurityNetty4HttpServerTransport] [node-2] publish_address {10.102.162
.14:9123}, bound_addresses {10.102.162.14:9123}
[2019-12-04T14:49:50,634][INFO ][o.e.n.Node               ] [node-2] started
[2019-12-04T19:31:35,723][INFO ][o.e.c.s.ClusterSettings  ] [node-2] updating [xpack.monitoring.collection.enabled]
from [false] to [true]
```
我怀疑是网络问题，导致发生了脑裂。日志(./logs/my-audit_access.log)太多了,不太好分析，没有error，有warning、failed之类的关键字。只能慢慢找，看上面的日志，中间有```reason(transport disconnected), zen-disco-node-failed({node-3}```之类的日志。
## logstash配置文件中写入的那个es的那个节点的日志
我查看了一下脱离集群的那个节点，发现上午反复进行测试的报错日志，都在es的日志里，而不是logstash里，卧槽了。。。。
其中看到了（当前节点是node1）：
```
Caused by: org.elasticsearch.transport.RemoteTransportException: [node-3][192.168.161.23:9399][internal:cluster/nodes
/indices/shard/store[n]]
Caused by: org.elasticsearch.ElasticsearchException: Failed to list store metadata for shard [[audit-paas-2019.12][4]]

[2019-12-03T15:56:08,467][DEBUG][o.e.a.a.i.s.TransportIndicesStatsAction] [node-1] failed to execute [indices:monito
r/stats] on node [arPYw1QzR5uyk6hcanQujw]
org.elasticsearch.transport.NodeNotConnectedException: [node-3][192.168.161.23:9399] Node not connected

[2019-12-03T15:56:09,120][WARN ][o.e.c.NodeConnectionsService] [node-1] failed to connect to node {node-2}{Ci-aSglBR
KWZM5JkoKvKrg}{wZVFRpTxTf2S6YScHcThwQ}{192.168.162.14}{192.168.162.14:9399}{ml.machine_memory=8252608512, ml.max_open_
jobs=20, xpack.installed=true, ml.enabled=true} (tried [1] times)
org.elasticsearch.transport.ConnectTransportException: [node-2][192.168.162.14:9399] connect_exception
```
## 排障技巧
* 其实当出现异常的时候，首先就应该检查集群状态、查看日志。
* kibana有个monitoring的菜单，里面后比较详细的监控，以前从来没点开过。。。

## 优化方案
* 合理设置节点和master节点，避免脑裂；
* 设置集群恢复的时间，避免断网几秒就开始恢复
gateway.recover_after_time: 5m
5分钟后才开始恢复。
